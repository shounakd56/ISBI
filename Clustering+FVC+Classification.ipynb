{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc2021db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.0 MB 8.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "\u001b[K     |████████████████████████████████| 450 kB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.6.2)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n",
      "\u001b[K     |████████████████████████████████| 436 kB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.21,>=0.20\n",
      "  Downloading tokenizers-0.20.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.9)\n",
      "Installing collected packages: huggingface-hub, tokenizers, safetensors, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.18.0\n",
      "    Uninstalling huggingface-hub-0.18.0:\n",
      "      Successfully uninstalled huggingface-hub-0.18.0\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.4.0\n",
      "    Uninstalling safetensors-0.4.0:\n",
      "      Successfully uninstalled safetensors-0.4.0\n",
      "Successfully installed huggingface-hub-0.27.0 safetensors-0.4.5 tokenizers-0.20.3 transformers-4.46.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc45eabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7b1ffd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1375/1279232707.py:24: DeprecationWarning: Please use `map_coordinates` from the `scipy.ndimage` namespace, the `scipy.ndimage.interpolation` namespace is deprecated.\n",
      "  from scipy.ndimage.interpolation import map_coordinates\n",
      "/tmp/ipykernel_1375/1279232707.py:25: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
      "  from scipy.ndimage.filters import gaussian_filter\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.collections import PatchCollection\n",
    "from shapely.geometry import Polygon, Point\n",
    "import sys\n",
    "import os\n",
    "import statistics\n",
    "import psutil\n",
    "import copy\n",
    "import pickle\n",
    "import re\n",
    "from abc import ABC, abstractmethod\n",
    "import math\n",
    "import copy\n",
    "import xml.etree.ElementTree as ET\n",
    "import gc\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from skimage import data\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.color import rgb2gray\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import skimage\n",
    "import PIL\n",
    "from skimage.filters import threshold_otsu\n",
    "import torchvision.models as torchmodels\n",
    "from torchvision.models import convnext_tiny,ConvNeXt_Tiny_Weights\n",
    "\n",
    "from torchvision.models import regnet_y_3_2gf,RegNet_Y_3_2GF_Weights\n",
    "\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "import abc\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score\n",
    "\n",
    "# from generativemodels import ResidualEncoder\n",
    "class FishVectEncoder(nn.Module):\n",
    "    def __init__(self, size_input, num_centers,\\\n",
    "                        pi_of_fishvect, sigma_of_fishvect):\n",
    "        '''\n",
    "        Inputs.\n",
    "            - size_input: the size of the input to the module, e.g., [32x100x7x7].\n",
    "            - num_centers: an integer, number of visual words.\n",
    "            - pi_of_fishvect: float, the pi value of the FishVect, Eq.34 of the survey.\n",
    "            - sigma_of_fishvect: float, the sigma value of the FishVect, Eq.34 of the survey.\n",
    "        '''\n",
    "        super(FishVectEncoder, self).__init__()\n",
    "        #grab the privates =====\n",
    "        self.num_centers = num_centers\n",
    "        self.pi_of_fishvect = pi_of_fishvect\n",
    "        self.sigma_of_fishvect = sigma_of_fishvect\n",
    "        #make V, the visualwords =====\n",
    "        NCHW = size_input\n",
    "        dim_vectors = NCHW[1]\n",
    "        self.V = torch.nn.Parameter(\n",
    "                      torch.randn(self.num_centers, dim_vectors).\\\n",
    "                      unsqueeze(-1).unsqueeze(-1).unsqueeze(0)\n",
    "                    ) #[1 x 11 x 2048 x 1 x 1]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Inputs.\n",
    "            - x: volumetric tensor of shape [20 x 2048 x 16 x 16]\n",
    "        Outputs.\n",
    "            - soft_assignment: [20 x 11 x 16 x 16]\n",
    "            - FV_mu_part: [20 x 11*2048  x 16 x 16]\n",
    "            - FV_sigma_part: #[20 x 11*2048  x 16 x 16]\n",
    "        '''\n",
    "        #make tao (Eq.35 of the survey) ===================\n",
    "        x_resized = x.unsqueeze(1) #[N x 1 x 2048 x 16 x 16]\n",
    "        x_min_V = x_resized - self.V.detach() #[N x 11 x 2048 x 16 x 16]\n",
    "        tao_exponent = torch.sum(\n",
    "                        -(x_min_V*x_min_V)/(2.0*self.sigma_of_fishvect*\\\n",
    "                                                self.sigma_of_fishvect),\\\n",
    "                        2\n",
    "                    ) #[N x 11 x 16 x 16]\n",
    "        soft_assignment = torch.nn.functional.softmax(\n",
    "                                    tao_exponent,\\\n",
    "                                    1\n",
    "                                  ) #[N x 11 x 16 x 16]\n",
    "        #make FV_mu_part =======\n",
    "        FV_mu_part = (x_min_V/self.sigma_of_fishvect) *\\\n",
    "                      soft_assignment.unsqueeze(2)*\\\n",
    "                      (1.0/np.sqrt(self.pi_of_fishvect))#[N x 11 x 2048 x 94 x 94]\n",
    "        list_dim = list(FV_mu_part.size())\n",
    "        new_list_dim = [list_dim[0]] + [-1] + list_dim[3:5]\n",
    "        FV_mu_part = FV_mu_part.view(*new_list_dim) #[N x 11*2048  x 94 x 94]\n",
    "        #make FV_sigma_part ======\n",
    "        FV_sigma_part =\\\n",
    "           ((x_min_V/self.sigma_of_fishvect)*\\\n",
    "            (x_min_V/self.sigma_of_fishvect) - 1) *\\\n",
    "             soft_assignment.unsqueeze(2)*\\\n",
    "             (1.0/np.sqrt(2.0*self.pi_of_fishvect))#[N x 11 x 2048 x 94 x 94]\n",
    "        FV_sigma_part = FV_sigma_part.view(*new_list_dim) #[N x 11*2048 x 94 x 94]\n",
    "        encoded_descriptors = torch.cat([FV_mu_part, FV_sigma_part],\\\n",
    "                                        1)\n",
    "        \n",
    "        return soft_assignment, encoded_descriptors\n",
    "fve = FishVectEncoder(\n",
    "                         size_input=[1,504,9],\n",
    "                         num_centers=5,pi_of_fishvect=0.2, sigma_of_fishvect=0.1\n",
    "                      )\n",
    "# [80,3,512,512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f3d46b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [[-1.1797481161434864, -0.545881941309972, -0....\n",
       "1      [[0.2269251048054033, 1.1229929524691435, 0.85...\n",
       "2      [[1.1918291726071466, 0.14175776077911373, 2.8...\n",
       "3      [[2.3236311513910355, 1.693732421254974, 2.790...\n",
       "4      [[1.162273544160464, 1.2154657209409305, 1.370...\n",
       "                             ...                        \n",
       "152    [[-0.6231923308293462, 0.5347237358815037, -0....\n",
       "153    [[1.3145381200127018, 1.6229865244096442, 0.13...\n",
       "154    [[0.8034674624606797, 0.0010932752042546179, 1...\n",
       "155    [[1.3603257818013987, 0.5504098856545874, 0.55...\n",
       "156    [[2.184599875414793, 0.4885522672763991, 0.609...\n",
       "Name: feat, Length: 157, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"efficient.csv\")\n",
    "import json\n",
    "df.feat = df.feat.apply(lambda x:json.loads(x))\n",
    "df[\"feat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19c09630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_inner_lists(feat_list):\n",
    "    return [lst for lst in feat_list if len(lst) != 8]\n",
    "df['feat'] = df['feat'].apply(filter_inner_lists)\n",
    "i=0\n",
    "for f in df[\"feat\"]:\n",
    "    for k in f:\n",
    "        if len(k)==8:\n",
    "#             print(i)\n",
    "            i=i+1\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "945ddfb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([504, 9])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(df[\"feat\"][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f115abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5040, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fve(torch.tensor(df[\"feat\"][32]))[1].squeeze(0).squeeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e32ca36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>feat</th>\n",
       "      <th>lab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[[-1.1797481161434864, -0.545881941309972, -0....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[[0.2269251048054033, 1.1229929524691435, 0.85...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[[1.1918291726071466, 0.14175776077911373, 2.8...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[[2.3236311513910355, 1.693732421254974, 2.790...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[[1.162273544160464, 1.2154657209409305, 1.370...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>152</td>\n",
       "      <td>[[-0.6231923308293462, 0.5347237358815037, -0....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>153</td>\n",
       "      <td>[[1.3145381200127018, 1.6229865244096442, 0.13...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>154</td>\n",
       "      <td>[[0.8034674624606797, 0.0010932752042546179, 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>155</td>\n",
       "      <td>[[1.3603257818013987, 0.5504098856545874, 0.55...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>156</td>\n",
       "      <td>[[2.184599875414793, 0.4885522672763991, 0.609...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                               feat  lab\n",
       "0             0  [[-1.1797481161434864, -0.545881941309972, -0....    1\n",
       "1             1  [[0.2269251048054033, 1.1229929524691435, 0.85...    0\n",
       "2             2  [[1.1918291726071466, 0.14175776077911373, 2.8...    0\n",
       "3             3  [[2.3236311513910355, 1.693732421254974, 2.790...    1\n",
       "4             4  [[1.162273544160464, 1.2154657209409305, 1.370...    0\n",
       "..          ...                                                ...  ...\n",
       "152         152  [[-0.6231923308293462, 0.5347237358815037, -0....    1\n",
       "153         153  [[1.3145381200127018, 1.6229865244096442, 0.13...    0\n",
       "154         154  [[0.8034674624606797, 0.0010932752042546179, 1...    1\n",
       "155         155  [[1.3603257818013987, 0.5504098856545874, 0.55...    1\n",
       "156         156  [[2.184599875414793, 0.4885522672763991, 0.609...    1\n",
       "\n",
       "[157 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ed118bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clus = pd.DataFrame(index=range(157), columns=['feat', 'lab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "888283e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1375/2785553540.py:26: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  cluster_mean=fve(torch.tensor(cluster_data))[1].squeeze(0).squeeze(1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "df_clus = pd.DataFrame(index=range(157), columns=['feat', 'lab'])\n",
    "for ab in range(157):\n",
    "    \n",
    "    num_clusters = 10\n",
    "    data=(df['feat'][ab])\n",
    "\n",
    "    data_array = np.array(data)\n",
    "    num_clusters = 10\n",
    "\n",
    "    # Perform k-means clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(data_array)\n",
    "\n",
    "    cluster_assignments = kmeans.predict(data_array)\n",
    "    cluster_means = []\n",
    "    for cluster_label in range(num_clusters):\n",
    "        cluster_data = [data_array[i] for i in range(len(cluster_assignments)) if cluster_assignments[i] == cluster_label]\n",
    "#         print(len(cluster_data),len(cluster_data[0]))\n",
    "#         cluster_mean = np.mean(cluster_data, axis=0)\n",
    "        fve = FishVectEncoder(\n",
    "                         size_input=[1,len(cluster_data),9],\n",
    "                         num_centers=7,pi_of_fishvect=0.1428571428, sigma_of_fishvect=0.1\n",
    "                      )\n",
    "        cluster_mean=fve(torch.tensor(cluster_data))[1].squeeze(0).squeeze(1)\n",
    "        cluster_means.append(cluster_mean)\n",
    "#     print(len(cluster_means),len(cluster_means[0]))\n",
    "#     print(cluster_means)\n",
    "    df_clus['feat'][ab]=cluster_means\n",
    "    df_clus['lab'][ab]=df['lab'][ab]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4152746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len((df_clus[\"feat\"][100])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6140ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.M = 500\n",
    "        self.L = 128\n",
    "        self.ATTENTION_BRANCHES = 1\n",
    "\n",
    "        self.feature_extractor_part1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 20, kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(20, 50, kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.feature_extractor_part2 = nn.Sequential(\n",
    "            nn.Linear(50 * 4 * 4, self.M),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.feature_extractor_part5 = nn.Sequential(\n",
    "            nn.Linear(1450, self.M),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.M, self.L), # matrix V\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.L, self.ATTENTION_BRANCHES) # matrix w (or vector w if self.ATTENTION_BRANCHES==1)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.M*self.ATTENTION_BRANCHES, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(0)\n",
    "\n",
    "        H = self.feature_extractor_part1(x)\n",
    "        if H.shape==torch.Size([32, 50, 1, 1]):\n",
    "            H = H.view(-1, 50 * 4 * 4)\n",
    "            H = self.feature_extractor_part2(H)  # KxM\n",
    "\n",
    "        else:\n",
    "            H = H.view(-1, 1450)\n",
    "            H = self.feature_extractor_part5(H)  # KxM\n",
    "\n",
    "\n",
    "        A = self.attention(H)  # KxATTENTION_BRANCHES\n",
    "        A = torch.transpose(A, 1, 0)  # ATTENTION_BRANCHESxK\n",
    "        A = F.softmax(A, dim=1)  # softmax over K\n",
    "\n",
    "        Z = torch.mm(A, H)  # ATTENTION_BRANCHESxM\n",
    "\n",
    "        Y_prob = self.classifier(Z)\n",
    "        Y_hat = torch.ge(Y_prob, 0.5).float()\n",
    "\n",
    "        return Y_prob, Y_hat, A\n",
    "\n",
    "    # AUXILIARY METHODS\n",
    "    def calculate_classification_error(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        _, Y_hat, _ = self.forward(X)\n",
    "        error = 1. - Y_hat.eq(Y).cpu().float().mean().data.item()\n",
    "\n",
    "        return error, Y_hat\n",
    "\n",
    "    def calculate_objective(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        Y_prob, _, A = self.forward(X)\n",
    "        Y_prob = torch.clamp(Y_prob, min=1e-5, max=1. - 1e-5)\n",
    "        neg_log_likelihood = -1. * (Y * torch.log(Y_prob) + (1. - Y) * torch.log(1. - Y_prob))  # negative log bernoulli\n",
    "\n",
    "        return neg_log_likelihood, A\n",
    "\n",
    "class GatedAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GatedAttention, self).__init__()\n",
    "        self.M = 500\n",
    "        self.L = 128\n",
    "        self.ATTENTION_BRANCHES = 1\n",
    "\n",
    "        self.feature_extractor_part1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 20, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(20, 50, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.feature_extractor_part2 = nn.Sequential(\n",
    "            nn.Linear(50 * 4 * 4, self.M),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.feature_extractor_part5 = nn.Sequential(\n",
    "            nn.Linear(1450, self.M),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.feature_extractor_part6 = nn.Sequential(\n",
    "            nn.Linear(50, self.M),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.attention_V = nn.Sequential(\n",
    "            nn.Linear(self.M, self.L), # matrix V\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.attention_U = nn.Sequential(\n",
    "            nn.Linear(self.M, self.L), # matrix U\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.attention_w = nn.Linear(self.L, self.ATTENTION_BRANCHES) # matrix w (or vector w if self.ATTENTION_BRANCHES==1)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.M*self.ATTENTION_BRANCHES, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(0)\n",
    "        H = self.feature_extractor_part1(x)\n",
    "\n",
    "        if H.shape==torch.Size([32, 50, 1, 1]):\n",
    "            H = H.view(-1, 50 * 4 * 4)\n",
    "            H = self.feature_extractor_part2(H)  # KxM\n",
    "\n",
    "        else:\n",
    "#             H = H.view(-1, 1450)\n",
    "            H = H.view(-1, 50)\n",
    "\n",
    "#             H = self.feature_extractor_part5(H)  # KxM\n",
    "            H = self.feature_extractor_part6(H)  # KxM\n",
    "\n",
    "        A_V = self.attention_V(H)  # KxL\n",
    "        A_U = self.attention_U(H)  # KxL\n",
    "        A = self.attention_w(A_V * A_U) # element wise multiplication # KxATTENTION_BRANCHES\n",
    "        A = torch.transpose(A, 1, 0)  # ATTENTION_BRANCHESxK\n",
    "        A = F.softmax(A, dim=1)  # softmax over K\n",
    "\n",
    "        Z = torch.mm(A, H)  # ATTENTION_BRANCHESxM\n",
    "\n",
    "        Y_prob = self.classifier(Z)\n",
    "        Y_hat = torch.ge(Y_prob, 0.5).float()\n",
    "\n",
    "        return Y_prob, Y_hat, A\n",
    "\n",
    "    # AUXILIARY METHODS\n",
    "    def calculate_classification_error(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        _, Y_hat, _ = self.forward(X)\n",
    "        error = 1. - Y_hat.eq(Y).cpu().float().mean().item()\n",
    "\n",
    "        return error, Y_hat\n",
    "\n",
    "    def calculate_objective(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        Y_prob, _, A = self.forward(X)\n",
    "        Y_prob = torch.clamp(Y_prob, min=1e-5, max=1. - 1e-5)\n",
    "        neg_log_likelihood = -1. * (Y * torch.log(Y_prob) + (1. - Y) * torch.log(1. - Y_prob))  # negative log bernoulli\n",
    "\n",
    "        return neg_log_likelihood, A\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GatedAttention1(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.02, weight_decay=1e-5):\n",
    "        super(GatedAttention1, self).__init__()\n",
    "        self.M = 50\n",
    "        self.L = 128\n",
    "        self.ATTENTION_BRANCHES = 1\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        # Feature extractor\n",
    "        self.feature_extractor_part1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 20, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(20, 50, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.feature_extractor_part2 = nn.Sequential(\n",
    "            nn.Linear(50 * 4 * 4, self.M),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention_V = nn.Sequential(\n",
    "            nn.Linear(self.M, self.L),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=self.dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.attention_U = nn.Sequential(\n",
    "            nn.Linear(self.M, self.L),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=self.dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.attention_w = nn.Linear(self.L, self.ATTENTION_BRANCHES)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.M * self.ATTENTION_BRANCHES, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "#         x=x.expand(-1, -1, -1, 4)\n",
    "#         x=x.expand(-1,4,-1)\n",
    "\n",
    "#         x = x.squeeze(0)\n",
    "#         print(x.shape)\n",
    "        H = self.feature_extractor_part1(x)\n",
    "\n",
    "        if H.shape == torch.Size([32, 50, 1, 1]):\n",
    "            H = H.view(-1, 50 * 4 * 4)\n",
    "            H = self.feature_extractor_part2(H)  # KxM\n",
    "\n",
    "        else:\n",
    "            H = H.view(-1, 50)\n",
    "\n",
    "        A_V = self.attention_V(H)  # KxL\n",
    "        A_U = self.attention_U(H)  # KxL\n",
    "        A = self.attention_w(A_V * A_U)\n",
    "        A = torch.transpose(A, 1, 0)  # ATTENTION_BRANCHESxK\n",
    "        A = F.softmax(A, dim=1)  # softmax over K\n",
    "\n",
    "        Z = torch.mm(A, H)  # ATTENTION_BRANCHESxM\n",
    "\n",
    "        Y_prob = self.classifier(Z)\n",
    "        Y_hat = torch.ge(Y_prob, 0.5).float()\n",
    "\n",
    "        return Y_prob, Y_hat, A\n",
    "\n",
    "    # AUXILIARY METHODS\n",
    "    def calculate_classification_error(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        _, Y_hat, _ = self.forward(X)\n",
    "        error = 1. - Y_hat.eq(Y).cpu().float().mean().item()\n",
    "\n",
    "        return error, Y_hat\n",
    "\n",
    "    def calculate_objective(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        Y_prob, _, A = self.forward(X)\n",
    "        Y_prob = torch.clamp(Y_prob, min=1e-5, max=1. - 1e-5)\n",
    "        neg_log_likelihood = -1. * (Y * torch.log(Y_prob) + (1. - Y) * torch.log(1. - Y_prob))\n",
    "\n",
    "        return neg_log_likelihood, A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f889c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# (np.vstack(df_clus[\"feat\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "841e99b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(157):\n",
    "    df_clus[\"feat\"][i]=np.vstack(df_clus[\"feat\"][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb54f480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:6\n",
      "0 48.226950354609926\n",
      "1 51.06382978723404\n",
      "2 58.156028368794324\n",
      "3 63.829787234042556\n",
      "4 63.829787234042556\n",
      "5 65.95744680851064\n",
      "6 65.95744680851064\n",
      "7 73.75886524822694\n",
      "8 72.3404255319149\n",
      "9 70.2127659574468\n",
      "10 77.30496453900709\n",
      "11 78.01418439716312\n",
      "12 77.30496453900709\n",
      "13 75.177304964539\n",
      "14 84.39716312056737\n",
      "1 141\n",
      "Precision: 0.8545\n",
      "Recall: 0.8440\n",
      "F1 Score: 0.8401\n",
      "AUC: 0.8275\n",
      "Test: 81.25\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import numpy as np\n",
    "\n",
    "X = torch.tensor(df_clus['feat'].tolist(), dtype=torch.float32).unsqueeze(1)\n",
    "y = torch.tensor(df_clus['lab'], dtype=torch.float32)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "device=\"cuda:6\"\n",
    "model = GatedAttention1()  # or GatedAttention()\n",
    "model.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "print(device)\n",
    "num_epochs = 8000\n",
    "abc=0\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    train_error = 0.\n",
    "    a=[]\n",
    "    p=[]\n",
    "    for inputs, labels in train_loader:\n",
    "\n",
    "        inputs=inputs.to(device)\n",
    "        labels=labels.to(device)\n",
    "        a.append(labels)\n",
    "        optimizer.zero_grad()\n",
    "      #  print(inputs.shape)\n",
    "     #   print(labels.shape)\n",
    "\n",
    "        loss, _ = model.calculate_objective(inputs, labels)\n",
    "#         print(loss)\n",
    "#         print(inputs.shape)\n",
    "        zeros = torch.zeros(3, dtype=loss.data[0].dtype)\n",
    "        if loss.data[0].shape==torch.Size([29]):\n",
    "            abcd = torch.cat((loss.data[0], zeros))\n",
    "            train_loss +=abcd\n",
    "\n",
    "        else:\n",
    "                train_loss += loss.data[0]\n",
    "\n",
    "\n",
    "        error, pred = model.calculate_classification_error(inputs, labels)\n",
    "        p.append(pred)\n",
    "        train_error += error\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      #  print(pred)\n",
    "        correct_predictions = (pred == labels).sum().item()\n",
    "        total_samples = len(labels)\n",
    "        accuracy = correct_predictions / total_samples\n",
    "    #print(p)\n",
    "    total_elements = len(a)\n",
    "    correct_count = sum(1 for i in range(total_elements) if a[i].item() == p[i].item())\n",
    "\n",
    "    accuracy = (correct_count / total_elements) * 100\n",
    "    a_numpy = np.array([tensor.item() for tensor in a])\n",
    "    p_numpy = np.array([tensor.item() for tensor in p])\n",
    "\n",
    "\n",
    "    print(epoch,accuracy)\n",
    "    if accuracy>80:\n",
    "\n",
    "        a_flat=a_numpy\n",
    "        p_flat=p_numpy\n",
    "\n",
    "        correct_predictions = (a_flat == p_flat)[0].sum().item()\n",
    "        total_samples = len(a)\n",
    "        accuracy =correct_predictions / total_samples\n",
    "        print(correct_predictions,total_samples)\n",
    "        precision = precision_score(a_flat, p_flat, average='weighted')\n",
    "        recall = recall_score(a_flat, p_flat, average='weighted')\n",
    "        f1 = f1_score(a_flat, p_flat, average='weighted')\n",
    "        auc = roc_auc_score(a_flat, p_flat, average='weighted', multi_class='ovo')\n",
    "        print(f'Precision: {precision:.4f}')\n",
    "        print(f'Recall: {recall:.4f}')\n",
    "        print(f'F1 Score: {f1:.4f}')\n",
    "        print(f'AUC: {auc:.4f}')\n",
    "        abc=abc+1\n",
    "        if abc>0:\n",
    "            break\n",
    "\n",
    "    \n",
    "model.eval()\n",
    "test_loss = 0.\n",
    "test_error = 0.\n",
    "    \n",
    "with torch.no_grad():\n",
    "    a=[]\n",
    "    p=[]\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "#             print(labels)\n",
    "            inputs=inputs.to(device)\n",
    "            labels=labels.to(device)\n",
    "            a.append(labels)\n",
    "\n",
    "            loss, attention_weights = model.calculate_objective(inputs, labels)\n",
    "            test_loss += loss.data[0]\n",
    "            error, predicted_label = model.calculate_classification_error(inputs, labels)\n",
    "            test_error += error\n",
    "            p.append(predicted_label)\n",
    "            correct_predictions = (predicted_label == labels).sum().item()\n",
    "            total_samples = len(labels)\n",
    "            accuracy = correct_predictions / total_samples\n",
    "#     print(a)\n",
    "#     print(p)\n",
    "\n",
    "\n",
    "    total_elements = len(a)\n",
    "    correct_count = sum(1 for i in range(total_elements) if a[i].item() == p[i].item())\n",
    "\n",
    "    accuracy = (correct_count / total_elements) * 100\n",
    "    print(\"Test:\",accuracy)\n",
    "    a_numpy = np.array([tensor.item() for tensor in a])\n",
    "    p_numpy = np.array([tensor.item() for tensor in p])\n",
    "    test_error /= len(test_loader)\n",
    "    test_loss /= len(test_loader)\n",
    "#     print(p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
